{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+ODUckfx4ZuKcnoKkhlbi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arul7441/Guvi-Project/blob/main/Guvi_Hate_Speech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LxxXQuItCjF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('C:\\Users\\Dell\\Desktop\\New folder\\Annotations_Metadata.csv'\n",
        "'C:\\Users\\Dell\\Desktop\\New folder\\Text file')\n",
        "pip install matplotlib\n",
        "# Check the first few rows of the dataset\n",
        "print(df.head())\n",
        "\n",
        "# Check the data types of columns\n",
        "print(df.dtypes)\n",
        "\n",
        "# Check for missing values\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Visualize data distributions (example using matplotlib)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(df['label'])\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Labels')\n",
        "plt.show()\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Clean the text data\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^A-Za-z]', ' ', text)  # Remove non-alphabetic characters\n",
        "    text = text.lower()  # Convert text to lowercase\n",
        "    return text\n",
        "\n",
        "df['cleaned_text'] = df['file_id'].apply(clean_text)\n",
        "\n",
        "# Tokenization\n",
        "df['tokens'] = df['cleaned_text'].apply(word_tokenize)\n",
        "\n",
        "# Stopword Removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "df['stemmed_tokens'] = df['tokens'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Count word frequencies\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(df['cleaned_text'])\n",
        "# Convert date/time columns to datetime objects\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "\n",
        "# Visualize trends over time (example using line plot)\n",
        "plt.plot(df['timestamp'], df['label'])\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('Label')\n",
        "plt.title('Hate Speech Trends Over Time')\n",
        "plt.show()\n",
        "# Group data by user_id\n",
        "user_groups = df.groupby('user_id')\n",
        "\n",
        "# Analyze user behavior\n",
        "user_stats = user_groups['label'].value_counts()\n",
        "# Assuming you have location data available in the dataset\n",
        "# Create geospatial visualizations using libraries like geopandas or folium\n",
        "# For example, you can create a folium map:\n",
        "import folium\n",
        "\n",
        "# Create a map centered at a specific location\n",
        "m = folium.Map(location=[latitude, longitude], zoom_start=10)\n",
        "\n",
        "# Add markers for hate speech occurrences\n",
        "for _, row in df.iterrows():\n",
        "    folium.Marker([row['latitude'], row['longitude']], popup=row['file_id']).add_to(m)\n",
        "\n",
        "# Save the map as an HTML file\n",
        "m.save('geospatial_analysis.html')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    }
  ]
}